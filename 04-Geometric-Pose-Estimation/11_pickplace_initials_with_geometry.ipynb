{"cells":[{"cellId":"84ffd9e2fa634dc6b67d896013d06e8a","cell_type":"markdown","metadata":{"vscode":{"languageId":"plaintext"},"cell_id":"84ffd9e2fa634dc6b67d896013d06e8a","deepnote_cell_type":"markdown"},"source":"# Pick And Place With Geometry\n\nIn [Exercise 3.10](https://manipulation.csail.mit.edu/pick.html#section11), [Exercise 3.14](https://manipulation.csail.mit.edu/pick.html#section11), [Exercise 4.6](https://manipulation.csail.mit.edu/pose.html#exercises),  we designed various trajectories for the robot arm to follow, created a `DifferentialIKSystem`, used both pseudoinverse and optimization-based controllers to move the robot arm and used ICP to register point-clouds for object meshes from the scene cameras. In this notebook, we will combine these tools together to build an entire pick and place system based on the previous exercises that pick up your initials.\n\n*Learning Objectives:*\nBuild an end to end system that  \n1. Sets up a scenario with arbitrary meshes\n2. Uses scene cameras and ICP to register the geometry of the objects\n3. Uses a Differential-IK controller to do pick and place.\n\n*What You'll Build:* An end to end pick and place system, featuring your intiials from Notebook 2. \n\n*Reference:* [Mustard Bottle Pick and Place](https://deepnote.com/workspace/Manipulation-ac8201a1-470a-4c77-afd0-2cc45bc229ff/project/cc6340f5-374e-449a-a195-839a3cedec4a/notebook/pose-c5a585e1a5fe4ebc8bc362c2a101857d)","block_group":"04111761ae6e4dd697b315b25d6458ae"},{"cellId":"b22e0a063ad848dd8d02edab1e544411","cell_type":"code","metadata":{"cell_id":"b22e0a063ad848dd8d02edab1e544411","deepnote_cell_type":"code"},"source":"import os\nfrom pathlib import Path\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport trimesh\nfrom pydrake.all import (\n    AddFrameTriadIllustration,\n    BasicVector,\n    Concatenate,\n    Context,\n    Diagram,\n    DiagramBuilder,\n    Integrator,\n    JacobianWrtVariable,\n    LeafSystem,\n    MultibodyPlant,\n    PiecewisePolynomial,\n    PiecewisePose,\n    PointCloud,\n    Rgba,\n    RigidTransform,\n    RobotDiagram,\n    RollPitchYaw,\n    RotationMatrix,\n    Simulator,\n    StartMeshcat,\n    Trajectory,\n    TrajectorySource,\n)\n\nfrom manipulation import running_as_notebook\nfrom manipulation.exercises.grader import Grader\nfrom manipulation.icp import IterativeClosestPoint\nfrom manipulation.letter_generation import create_sdf_asset_from_letter\nfrom manipulation.meshcat_utils import AddMeshcatTriad\nfrom manipulation.station import (\n    AddPointClouds,\n    LoadScenario,\n    MakeHardwareStation,\n    RobotDiagram,\n)\nfrom manipulation.utils import RenderDiagram","block_group":"6dea6e3ef11041509b3eeaa0756b7e94","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cellId":"a1eb7d0575cd4e5b89e3506f8dacb25d","cell_type":"markdown","metadata":{"cell_id":"a1eb7d0575cd4e5b89e3506f8dacb25d","deepnote_cell_type":"markdown"},"source":"# Meshcat Visualization\n\nAs always, we begin by starting Meshcat.","block_group":"4d91820cd79043258b8a1a1a56a229a9"},{"cellId":"896d40148baa49b5afa903c1c531f3ff","cell_type":"code","metadata":{"cell_id":"896d40148baa49b5afa903c1c531f3ff","deepnote_cell_type":"code"},"source":"# Start meshcat for visualization\nmeshcat = StartMeshcat()\nprint(\"Click the link above to open Meshcat in your browser!\")","block_group":"2630d4f63f524176943477028a1adcc8","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cellId":"41ad007a8367473db8f92c64d00641e2","cell_type":"markdown","metadata":{"cell_id":"41ad007a8367473db8f92c64d00641e2","deepnote_cell_type":"markdown"},"source":"# Part 1: Setting Up the Scenario\n\nIn [Exercise 1.4](https://manipulation.csail.mit.edu/intro.html#section7) from chapter 1 you created a scenario file for custom objects and used the `MakeHardwareStation` function and `LoadScenario` functions to create a simulation with your initials.\n\nIn this section, we'll repeat that setup so we can get started with our simulation. Unlike in that notebook, however, this time we will also add a set of scene cameras so we can also sense object locations. We'll make use of the `LoadScenario` function which will abstract away setting up the systems for all the cameras and the driver logic. In this section, we'll add onto this and create a more complicated scenario using all of the different types of functionalities of a scenario:\n* Directives for all the robot arms, tables, and cameras\n* Model drivers for all the robot arms so they can actually be actuated\n* Cameras with corresponding configs. \n\nBecause our simulation now consists of a lot of moving parts including the directives for both the robot arms and cameras, we will modularize the components into two parts: the directives and drivers/cameras. Then, simply running `LoadScenario` to get a `Scenario` config object and then running `MakeHardwareStation` will generate all the necessary systems for all the cameras which will allow us to generate the necessary point cloud geometry. \n\nFirst, we generate the initials and table assets again. For simplicity, in this notebook we will ask that you only generate one of your initials. In order to use the depth camera sensors to get the point clouds, we need to make sure our meshes also have all the normal information added (in real life all of this information is just available in the scene but in simulation these are properties that must be made available). Fortunately, this can be done by simply adding the`include_normal=True` argument to the original `create_sdf_assets_from_letter` function. Let's do that below:\n\n**YOUR TASK**: Regenerate the letters with normals, create a station with the robot, cameras, and depth sensors, and add the corresponding systems.\n\n**Key Concepts**: Using directives to append scenarios and adding point clouds to the system with `AddPointClouds` and `DepthImageToPointClouds`.","block_group":"b07e24028b97408f9dc143cf7c40e752"},{"cellId":"f8e5a37c47b242beac15518a58951085","cell_type":"code","metadata":{"cell_id":"f8e5a37c47b242beac15518a58951085","deepnote_cell_type":"code"},"source":"# A note: as an additional argument add `include_normals=True` to the `create_sdf_asset_from_letter` function.\n# We also recommend you use a slightly smaller `letter_height_meters=0.15` and a rather high friction coefficient\n# (we use mu_static = 1.17, mu_dynamic = 1.0 - roughly equal to the friction of rubber on granite) to make the\n# letters easier to pick up. We'll study more complicated grasping strategies in future notebooks, but this is not\n# the priority of this chapter so we'll cheat.\n\noutput_dir = Path(\"assets/\")\n\n# TODO: replace ADD_YOUR_INITIAL_HERE with your initial.\nyour_initial = \"ADD_YOUR_INITIAL_HERE\"\n\n# TODO: generate the SDF file for your intial.","block_group":"b664e3272a314611b40493d53016c616","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cellId":"4448c51e8f23405f90bd731eee7e7e51","cell_type":"code","metadata":{"cell_id":"4448c51e8f23405f90bd731eee7e7e51","deepnote_cell_type":"code"},"source":"# TODO: Paste your table_sdf = ... code from Exercise 1.4 here\n\n# TODO: Write the table SDF to assets/table.sdf","block_group":"bc993523f18f498387645776a29d7705","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cellId":"3df275595e2f4f65bb65b1b08919c430","cell_type":"markdown","metadata":{"cell_id":"3df275595e2f4f65bb65b1b08919c430","deepnote_cell_type":"markdown"},"source":"We'll now create a scenario with a Kuka IIWA arm, a table, and your initials along with a set of scene cameras. Because our scenario is becoming quite big, we'll first split up the scenario directives. We'll then load the directives and include the scene cameras and the drivers later in the scenario. As a reference, look at this [example](https://github.com/RussTedrake/manipulation/blob/3b74947270c02c7202cc33197d1e41d2cf34dea0/manipulation/models/clutter.scenarios.yaml) scenario file for an example of how to do so.\n\nOnce we have the correct scenario file, we can simply run `LoadScenario` and this will generate all of the camera systems. Unlike in [Exercise 3.14](https://manipulation.csail.mit.edu/pick.html#section11), we recommend adding around 3 scene cameras (the default settings in the example should work), but feel free to play around with the exact geometry of the cameras. \n\nAlso, play around a bit with the exact starting position (the `X_PC` of the `iiwa_link_0` relative to the world frame). Make it start in a reasonable position relative to the letters, although it doesn't have to be perfect as we use now use geometric sensing.","block_group":"244f3cf66c124c2dbb522c0ba83a7fca"},{"cellId":"931188a0328f4541921b9f1724707c65","cell_type":"code","metadata":{"cell_id":"931188a0328f4541921b9f1724707c65","deepnote_cell_type":"code"},"source":"# Add the directives for the bimanual IIWA arms, table, and initials\n\n\ndef generate_bimanual_IIWA14_with_assets_directives_file() -> (\n    tuple[Diagram, RobotDiagram]\n):\n    table_sdf = f\"{Path.cwd()}/assets/table.sdf\"\n    letter_sdf = f\"{Path.cwd()}/assets/{your_initial}_model/{your_initial}.sdf\"\n\n    directives_yaml = f\"\"\"directives:\n- add_model:\n    name: iiwa\n    file: package://drake_models/iiwa_description/sdf/iiwa7_no_collision.sdf\n    default_joint_positions:\n        iiwa_joint_1: [-1.57]\n        iiwa_joint_2: [0.1]\n        iiwa_joint_3: [0]\n        iiwa_joint_4: [-1.2]\n        iiwa_joint_5: [0]\n        iiwa_joint_6: [ 1.6]\n        iiwa_joint_7: [0]\n- add_weld:\n    parent: world\n    child: iiwa::iiwa_link_0\n    X_PC:\n        translation: [0, -0.5, 0]\n        rotation: !Rpy {{ deg: [0, 0, 180] }}\n- add_model:\n    name: wsg\n    file: package://manipulation/hydro/schunk_wsg_50_with_tip.sdf\n- add_weld:\n    parent: iiwa::iiwa_link_7\n    child: wsg::body\n    X_PC:\n        translation: [0, 0, 0.09]\n        rotation: !Rpy {{ deg: [90, 0, 90]}}\n- add_model:\n    name: table\n    file: file://{table_sdf}\n- add_weld:\n    parent: world\n    child: table::table_link\n    X_PC:\n        translation: [0.0, 0.0, -0.05]\n        rotation: !Rpy {{ deg: [0, 0, -90] }}\n- add_model:\n    name: {your_initial}_letter\n    file: file://{letter_sdf}\n    default_free_body_pose:\n        {your_initial}_body_link:\n            translation: [-0.35, 0, 0]\n            rotation: !Rpy {{ deg: [0, 0, 0] }}\n\"\"\"\n    os.makedirs(\"directives\", exist_ok=True)\n\n    with open(\n        \"directives/bimanual_IIWA14_with_table_and_initials_and_assets.dmd.yaml\", \"w\"\n    ) as f:\n        f.write(directives_yaml)\n\n\ngenerate_bimanual_IIWA14_with_assets_directives_file()","block_group":"affc147c5e4b4f5ebe2885290e93c834","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cellId":"32d18a8550b44f89ac7bca4e21e7d78a","cell_type":"markdown","metadata":{"cell_id":"32d18a8550b44f89ac7bca4e21e7d78a","deepnote_cell_type":"markdown"},"source":"We'll now add the camera directives into a seperate directive and then append them into a single scenario.","block_group":"b44a8d6dbd34422a9e9ebe9f428de93f"},{"cellId":"961b8788fdc24996840a0afd8798cca7","cell_type":"code","metadata":{"cell_id":"961b8788fdc24996840a0afd8798cca7","deepnote_cell_type":"code"},"source":"def create_camera_directives() -> None:\n    camera_directives_yaml = \"\"\"\ndirectives:\n- add_frame:\n    name: camera0_origin\n    X_PF:\n        base_frame: world\n        rotation: !Rpy { deg: [-120.0, 0.0, 180.0]}\n        translation: [0, 0.8, 0.5]\n\n- add_model:\n    name: camera0\n    file: package://manipulation/camera_box.sdf\n\n- add_weld:\n    parent: camera0_origin\n    child: camera0::base\n\n- add_frame:\n    name: camera1_origin\n    X_PF:\n        base_frame: world\n        rotation: !Rpy { deg: [-125, 0.0, 90.0]}\n        translation: [0.8, 0.1, 0.5]\n\n- add_model:\n    name: camera1\n    file: package://manipulation/camera_box.sdf\n\n- add_weld:\n    parent: camera1_origin\n    child: camera1::base\n\n- add_frame:\n    name: camera2_origin\n    X_PF:\n        base_frame: world\n        rotation: !Rpy { deg: [-120.0, 0.0, -90.0]}\n        translation: [-0.8, 0.1, 0.5]\n\n- add_model:\n    name: camera2\n    file: package://manipulation/camera_box.sdf\n\n- add_weld:\n    parent: camera2_origin\n    child: camera2::base\n\"\"\"\n    with open(\"directives/camera_directives.dmd.yaml\", \"w\") as f:\n        f.write(camera_directives_yaml)\n\n\ncreate_camera_directives()","block_group":"9c12b578b8e84f2282d36f8c7028d890","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cellId":"18643cf9f3434713b81a855860df3762","cell_type":"markdown","metadata":{"cell_id":"18643cf9f3434713b81a855860df3762","deepnote_cell_type":"markdown"},"source":"Next, we'll combine the directives, camera configs, and drivers into a single scenario file. Look back at the [clutter scenario](https://github.com/RussTedrake/manipulation/blob/3b74947270c02c7202cc33197d1e41d2cf34dea0/manipulation/models/clutter.scenarios.yaml) example to see how to do this.","block_group":"b110a7d56643484c9e5d1f3643822a44"},{"cellId":"45ac03813f214aa090045ca13428d386","cell_type":"code","metadata":{"cell_id":"45ac03813f214aa090045ca13428d386","deepnote_cell_type":"code"},"source":"def create_bimanual_IIWA14_with_assets_and_cameras_scenario() -> None:\n    # TODO: create a scenario yaml with the directives added with `add_directives`\n    scenario_yaml = \"\"\n    # TODO: add the camera configs and iiwa drivers with `add_cameras` and `add_iiwa_drivers`\n\n    os.makedirs(\"scenarios\", exist_ok=True)\n\n    with open(\n        \"scenarios/bimanual_IIWA14_with_table_and_initials_and_assets_and_cameras.scenario.yaml\",\n        \"w\",\n    ) as f:\n        f.write(scenario_yaml)\n\n\ncreate_bimanual_IIWA14_with_assets_and_cameras_scenario()","block_group":"f0c3cc3896ab497ab6ed36e33ecbbaf4","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cellId":"ee9c48ad075c49fbb5fcc571aafa0e25","cell_type":"markdown","metadata":{"cell_id":"ee9c48ad075c49fbb5fcc571aafa0e25","deepnote_cell_type":"markdown"},"source":"Now that we have the scenario file, we'll load it up into a DiagramBuilder using `MakeHardwareStation`. In order to get the point clouds for the meshes we will need to add the neccesary systems to the diagram. As in [Exercise 4.9](https://manipulation.csail.mit.edu/pose.html#exercises), we make use of the `AddPointClouds`function in `manipulation.station` which under the hood adds a `DepthImageToPointCloud` system to each camera (and connects it to the corresponding ports) in order to extract the corresponding point clouds. We will then need to export the output ports for this system to the output of the diagram allowing for these point clouds to be accesed for when we do ICP later.","block_group":"94d7c5b01a2048fe8a97c1c42213096e"},{"cellId":"e8b5accc7d7d40c987a2cbbdd2d151b0","cell_type":"code","metadata":{"cell_id":"e8b5accc7d7d40c987a2cbbdd2d151b0","deepnote_cell_type":"code"},"source":"def create_bimanual_IIWA14_with_table_and_initials_and_assets_and_cameras() -> (\n    tuple[DiagramBuilder, RobotDiagram]\n):\n    # TODO: Load the scenario created above into a Scenario object\n\n    # TODO: Create HardwareStation with the scenario and meshcat\n\n    # TODO: Make a DiagramBuilder, add the station, and build the diagram\n\n    # TODO: Add the point clouds to the diagram with AddPointClouds\n\n    # TODO: export the point cloud outputs to the builder\n\n    # TODO: Return the builder AND the station (notice that here we will need both)\n    raise NotImplementedError()","block_group":"d3b1b9c0a66f417784fcc9505960b116","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cellId":"bf7de7099f4b41e2b72e4a385e065ca1","cell_type":"markdown","metadata":{"cell_id":"bf7de7099f4b41e2b72e4a385e065ca1","deepnote_cell_type":"markdown"},"source":"Great! We now have a scenario with our robot, assets, and the cameras. Build them up and render a diagram fromo the builder below to check for any errors:","block_group":"9fd6f119b99845d28fe2a441a3c2079c"},{"cellId":"ba7de2f7c5aa49cea34a099eacb23478","cell_type":"code","metadata":{"cell_id":"ba7de2f7c5aa49cea34a099eacb23478","deepnote_cell_type":"code"},"source":"builder, station = (\n    create_bimanual_IIWA14_with_table_and_initials_and_assets_and_cameras()\n)\n\n# in order to debug, we will build the diagram once here.\ndiagram = builder.Build()\n\n# visualize the diagram\nRenderDiagram(diagram, max_depth=1)\n\n# publish the diagram with some default context\ncontext = diagram.CreateDefaultContext()\ndiagram.ForcedPublish(context)","block_group":"20135e8d669745089fb238ee47a6f196","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cellId":"88344f3b2bf74588b77a970879512ac3","cell_type":"markdown","metadata":{"cell_id":"88344f3b2bf74588b77a970879512ac3","deepnote_cell_type":"markdown"},"source":"# Part 2. Registering the Point Clouds With ICP\n\nIn the pose estimation ICP notebook, we cropped out the objects of interest and used the depth cameras to get a single camera's point cloud and then used ICP to register our object geometries. In this notebook, we'll take a slightly more realistic example: rather than using a single point cloud, we will make use of point clouds from multiple different cameras by merging them through concatenation (as a follow up, in a future exercise you will use RANSAC to remove outliers in the point cloud - in this notebook we simply merge them and call it a day). \n\nFinally, we'll make use of ICP to register the point cloud with our known geometries. Unlike in [Exercise 4.6](https://deepnote.com/workspace/Manipulation-ac8201a1-470a-4c77-afd0-2cc45bc229ff/project/cc6340f5-374e-449a-a195-839a3cedec4a/notebook/bunny_icp-300d196ed18d43f292fe396f8fbd5582?secondary-sidebar-autoopen=true&secondary-sidebar=agent), rather than coding ICP from scratch, we'll make use of the `IterativeClosestPoint` function in `manipulation.icp`. \n\n**Your Task**: segment out the object of interest and get the point clouds, merge the point clouds with concatenation, and then use ICP to register point cloud geometries.\n\n**Key Concepts**: Cropping out the point clouds, registering the point cloud geometries using the ICP function in `manipulation.icp`. \n\n**References**: [Point Cloud Processing Notebook](https://github.com/RussTedrake/manipulation/blob/c9f8d668de91241d170e3b6d692c4299e6f5e055/book/clutter/point_cloud_processing.ipynb#L45)","block_group":"1fde670deb0a4ed09244956c04e1b24f"},{"cellId":"4564d10a617e481da5719fb2aac85e20","cell_type":"markdown","metadata":{"cell_id":"4564d10a617e481da5719fb2aac85e20","deepnote_cell_type":"markdown"},"source":"## Getting the Point Cloud of the Model\n\nBefore running ICP, we need a point cloud of the *model* to compare against. To do this, we'll load into a `PointCloud` object the obj files corresponding to the letters. \n\nWe'll make use of the [trimesh](https://trimesh.org/) library to do this. Because our obj file contains the full 3d geometry (rather than a set of points for a point cloud) we will need to load the obj file as a mesh and then sample points from it to get a point cloud. ","block_group":"4a5b12252bd74ce28bf05e21878d56db"},{"cellId":"082f68ff02f94efabeee271ff5016bc3","cell_type":"code","metadata":{"cell_id":"082f68ff02f94efabeee271ff5016bc3","deepnote_cell_type":"code"},"source":"N_SAMPLE_POINTS = 1500\n\n# TODO: load your intitials with trimesh.load(...) as a mesh.\n# To do this, you should make sure to use the kwargs force=\"mesh\".\n# See the docs for more info at https://trimesh.org/\n\n# TODO: sample N_SAMPLE_POINTS from the mesh and then turn those points into a numpy array (you might need to transpose)\n# You should make use of the `sample` method of the mesh object.\n\n# TODO: create a `PointCloud` object from the numpy array","block_group":"497929dbdedf45f5b86f64824ae87048","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cellId":"a096f44ed1f24c12bbebf204d09f2d46","cell_type":"markdown","metadata":{"cell_id":"a096f44ed1f24c12bbebf204d09f2d46","deepnote_cell_type":"markdown"},"source":"## Cropping Out the Point Clouds\n\nNext, we'll get the camera point clouds. We'll start by cropping out the objects of interest within each camera. \n\nStart by visualizing the rgb and depth outputs of each of the rgbd sensors to make sure your camera is in the right position. If it's not, edit the camera rotation and/or translation so they roughly align with your initials. It doesn't have to be perfect as we'll segment out the objects later: the objects just have to lie in the frame. ","block_group":"246543197eac4e60adbd4c449367055b"},{"cellId":"bf5fc712a3234b4782d67ce7cf2914fd","cell_type":"code","metadata":{"cell_id":"bf5fc712a3234b4782d67ce7cf2914fd","deepnote_cell_type":"code"},"source":"# run the following cell to visualize the rgb outputs of each of the cameras\ncameras = [\"camera0\", \"camera1\", \"camera2\"]\nstation_context = diagram.GetSubsystemContext(station, context)\n\nfig, axes = plt.subplots(\n    1, len(cameras), figsize=(5 * len(cameras), 4), constrained_layout=True\n)\nfor ax, cam in zip(axes, cameras):\n    img = station.GetOutputPort(f\"{cam}.rgb_image\").Eval(station_context)\n    arr = np.array(img.data, copy=False).reshape(img.height(), img.width(), -1)\n    im = ax.imshow(arr)\n    ax.set_title(f\"{cam} rgb image\")\n    ax.axis(\"off\")\n\nif running_as_notebook:\n    plt.show()","block_group":"2af9547cb6c345249e27dae00bb312da","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cellId":"638b278eb27c4c15bf5fafbeed22d0ee","cell_type":"code","metadata":{"cell_id":"638b278eb27c4c15bf5fafbeed22d0ee","deepnote_cell_type":"code"},"source":"# run the following cell to visualize the depth outputs of each of the cameras\nfig, axes = plt.subplots(\n    1, len(cameras), figsize=(5 * len(cameras), 4), constrained_layout=True\n)\nfor ax, cam in zip(axes, cameras):\n    img = station.GetOutputPort(f\"{cam}.depth_image\").Eval(station_context)\n    depth_img = np.array(img.data, copy=False).reshape(img.height(), img.width(), -1)\n    depth_img = np.ma.masked_invalid(depth_img)\n    img = ax.imshow(depth_img, cmap=\"magma\")\n    ax.set_title(f\"{cam} depth image\")\n    ax.axis(\"off\")\n\nif running_as_notebook:\n    plt.show()","block_group":"8189f730bc6249daa1f27ea63cae3e10","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cellId":"264e1f8b8d0343e0b8cc20dcfd07b53f","cell_type":"markdown","metadata":{"cell_id":"264e1f8b8d0343e0b8cc20dcfd07b53f","deepnote_cell_type":"markdown"},"source":"Next, we'll get the point clouds from each of the three cameras and simply crop out the desired regions (the regions that contain the letters) based on position. Here, we're going to cheat a little bit and use the reference positions of the letters based on the cheat ports in the system to save you some time, but in the real world you should either manually change the positions of the bounding boxes. We'll also remove the table point cloud as the point clouds from the table are not of interest. \n\nFor more complicated scenarios, it might be helpful to use more interesting strategies for segmentation including color-based ones, outlier detection algorithms, end-to-end vision-based segmentation algorithms, and so forth. \n\nUse the [Mustard Bottle Point Cloud Processing](https://github.com/RussTedrake/manipulation/blob/c9f8d668de91241d170e3b6d692c4299e6f5e055/book/clutter/point_cloud_processing.ipynb#L75) and [Clutter Clearing](https://github.com/RussTedrake/manipulation/blob/c9f8d668de91241d170e3b6d692c4299e6f5e055/book/clutter/clutter_clearing.ipynb#L4) notebooks as an example.","block_group":"b1b9283dd8424424899ee168fc2f1b50"},{"cellId":"4a90b76ecee04988807e6ea92e52b7a5","cell_type":"code","metadata":{"cell_id":"4a90b76ecee04988807e6ea92e52b7a5","deepnote_cell_type":"code"},"source":"# calculate the actual relative positions of each of the letters to use as a reference\n# for cropping the point clouds.\nplant = station.plant()\nplant_context = diagram.GetSubsystemContext(plant, context)\n\nworld_frame = plant.world_frame()\n\nmodel_letter = plant.GetModelInstanceByName(f\"{your_initial}_letter\")\nframe_letter = plant.GetFrameByName(\n    f\"{your_initial}_body_link\", model_instance=model_letter\n)\nX_PC_letter = plant.CalcRelativeTransform(plant_context, world_frame, frame_letter)","block_group":"52a80563319c45e39d25ebc35eaddaaf","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cellId":"1d3de789824d4ddca75934117d439cbc","cell_type":"code","metadata":{"cell_id":"1d3de789824d4ddca75934117d439cbc","deepnote_cell_type":"code"},"source":"# TODO: get the point clouds from each of the cameras. For now, don't worry too much about\n# removing the table points as we'll do that later.\n\n# TODO: crop out each of the letters from each of the cameras\n# HINT: A simple way to do this is to use a simple Crop based on the X_PC_{letter} transforms given above.\n#       Add a small offset based on the size of your letters to make the whole letter fit in the crop.\n#       We add a visualization below to help you visualize the cropped point clouds.\n\nletter_lower = None  # TODO\nletter_upper = None  # TODO\n\ncamera0_letter_point_cloud = None  # TODO\ncamera1_letter_point_cloud = None  # TODO\ncamera2_letter_point_cloud = None  # TODO\n\n# This will visualize the bounding boxes for each of the letters in the point clouds\n# Use it to adjust your bounding boxes to fit the letters.\nif letter_lower is not None and letter_upper is not None:\n    meshcat.SetLineSegments(\n        \"bounding_line\",\n        np.array(letter_lower).T,\n        np.array(letter_upper).T,\n        1.0,\n        Rgba(0, 0, 0),\n    )\n\n# TODO: concatenate the point clouds\n\n# TODO: downsample the point clouds\n# HINT: use the `VoxelizedDownSample` method for PointCloud","block_group":"cf9659feb2b9415799f81ca740c6685d","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cellId":"b727a16978924d3da593aa3c16ae0a56","cell_type":"markdown","metadata":{"cell_id":"b727a16978924d3da593aa3c16ae0a56","deepnote_cell_type":"markdown"},"source":"Next, we'll subtract away the point clouds for all the points in the table. We'll cheat a little bit and use the fact that all points in the table lie below around z=0.01 so we'll remove all of those points. This is a bit of a \"fudge factor\" that makes use of our positioning/height of the table to segment out unneccesary points:","block_group":"00e753c5b3c7430d898ba83a799077a1"},{"cellId":"e2e9db237bd246ab9ee57eb42f4031b4","cell_type":"code","metadata":{"cell_id":"e2e9db237bd246ab9ee57eb42f4031b4","deepnote_cell_type":"code"},"source":"# TODO: implement a function that removes all points in the point cloud that lie below z=0.01\n\n\ndef remove_table_points(point_cloud: PointCloud) -> PointCloud:\n    raise NotImplementedError()\n\n\n# TODO: remove the table points from the concatenated point clouds","block_group":"0dc2984744cc41a69b343c0875ac8d19","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cellId":"d6de306f47374ab8be1339f7cf689445","cell_type":"code","metadata":{"cell_id":"d6de306f47374ab8be1339f7cf689445","deepnote_cell_type":"code"},"source":"# visualize the concatenated point clouds\nmeshcat.SetObject(\n    \"letter_point_cloud\", letter_point_cloud, point_size=0.05, rgba=Rgba(1, 0, 0)\n)","block_group":"e34dbf8fc4e44fd6b5a1b65d4ea77924","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cellId":"580dff2a49e74c79b5399f36367ad274","cell_type":"markdown","metadata":{"cell_id":"580dff2a49e74c79b5399f36367ad274","deepnote_cell_type":"markdown"},"source":"If everything is done correctly, you should be able to see point clouds for your intials that are created from your camera like below:\n\n![ps3_p4](https://raw.githubusercontent.com/RussTedrake/manipulation/master/book/figures/exercises/geometry_pick_and_place_point_clouds.png)\n\nNOTE: the things you should be visualizing should be `{letter}_source_cloud` NOT the `{letter}_model_source_cloud`s we created for reference for ICP.","block_group":"75aa7ae60d15404181f200c43ff96bcc"},{"cellId":"38c3aed9335f4ac7bc3d8740ac2f44b0","cell_type":"markdown","metadata":{"cell_id":"38c3aed9335f4ac7bc3d8740ac2f44b0","deepnote_cell_type":"markdown"},"source":"## Registering Point Cloud Geometry with ICP\n\nNow that we have the point clouds make use of the built in `IterativeClosestPoint` function to register the point cloud geometries. Use the [Pose Estimation ICP](https://github.com/RussTedrake/manipulation/blob/c9f8d668de91241d170e3b6d692c4299e6f5e055/book/pose/exercises/pose_estimation_icp.ipynb#L4) notebook as a reference. You should end up with a pose ","block_group":"d6ed743a1e6a41618a44ef56bc571504"},{"cellId":"97691d13b9564f4abf7b4785edaae1ef","cell_type":"code","metadata":{"cell_id":"97691d13b9564f4abf7b4785edaae1ef","deepnote_cell_type":"code"},"source":"MAX_ITERATIONS = 25 if running_as_notebook else 2\n\n# TODO: set initial guesses for each of the letters along with the maximum number of iterations\n# These can be rough guesses as ICP will do the bulk of the work of aligning the point clouds.\n\n# TODO: convert both the model and generated point clouds to numpy arrays to pass into the ICP function\n\n# TODO: register the point clouds with the model point clouds using ICP for each of the letters","block_group":"ad9df10e94e1470faa7507e81321a0c9","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cellId":"31e345940d304088979e769b665afc2f","cell_type":"code","metadata":{"cell_id":"31e345940d304088979e769b665afc2f","deepnote_cell_type":"code"},"source":"# check the error in the registration for each of the letters below:\n# if it has converged, all errors should be close to zero\nnp.set_printoptions(precision=3, suppress=True)\nerror_letter = letter_X_Ohat.inverse().multiply(X_PC_letter)\n\nrpy = RollPitchYaw(error_letter.rotation()).vector()\nxyz = error_letter.translation()\nprint(f\"{your_initial}_letter error: rpy: {rpy}, xyz: {xyz}\")","block_group":"9cb49081e84348a9a5893d49f234fcfd","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cellId":"4f75776994304193ad31c120e0417855","cell_type":"markdown","metadata":{"cell_id":"4f75776994304193ad31c120e0417855","deepnote_cell_type":"markdown"},"source":"# Part 3: Pick and Place with Registered Geometries\n\nPhew! Now that we finally have all the geometry figured out, we can start doing pick and place with the initials. For this part, we'll mainly just be reusing the code from the previous pset with some minor edits. We'll be using a pseudoinverse controller for the pick and place, but for practice feel free to use something more complicated like an optimization-based controller which might come useful for more complicated scenarios.\n\nWe'll follow the same general pattern as [Exercise 3.14](https://manipulation.csail.mit.edu/pick.html#section11): this time, you will be tasked with getting at least one of your initials upright (so that the z axis of the object and the z axis of the world are parallel). This time we'll add a few more steps so the general trajectory will follow the pattern initial -> prepick -> pick -> preplace -> place -> postplace -> initial.","block_group":"e9f3a960612a4ed89871a1d0621b25e7"},{"cellId":"f5b3d951fc9b4ec8ba9b7884a8f4763e","cell_type":"code","metadata":{"cell_id":"f5b3d951fc9b4ec8ba9b7884a8f4763e","deepnote_cell_type":"code"},"source":"# TODO: modify the functions below to work with your letters. This will require quite a bit of trial and error.\n\n\ndef design_grasp_pose(X_WO: RigidTransform) -> tuple[RigidTransform, RigidTransform]:\n    R_OG = (\n        RollPitchYaw(0, 0, np.pi).ToRotationMatrix()\n        @ RollPitchYaw(-np.pi / 2, 0, 0).ToRotationMatrix()\n    )\n    p_OG = [0.07, 0.08, 0.12]\n    X_OG = RigidTransform(R_OG, p_OG)\n    X_WG = X_WO.multiply(X_OG)\n    return X_OG, X_WG\n\n\ndef design_pregrasp_pose(\n    X_WG: RigidTransform,\n) -> tuple[RigidTransform, RigidTransform, RigidTransform]:\n    X_GGApproach = RigidTransform([0.0, -0.2, 0.0])\n    X_WGApproach = X_WG @ X_GGApproach\n    return X_WGApproach\n\n\ndef design_pregoal_pose(\n    X_WG: RigidTransform,\n) -> tuple[RigidTransform, RigidTransform, RigidTransform]:\n    X_GGApproach = RigidTransform([0.0, 0.0, -0.2])\n    X_WGApproach = X_WG @ X_GGApproach\n    return X_WGApproach\n\n\n# The goal poses have been modified to include the third initial.\ndef design_goal_poses(\n    X_WO: RigidTransform, X_OG: RigidTransform\n) -> tuple[RigidTransform, RigidTransform, RigidTransform]:\n    X_WOgoal = X_WO @ RigidTransform(\n        R=RotationMatrix.MakeXRotation(np.pi / 2), p=np.array([-0.1, 0.2, 0.03])\n    )\n    X_WGgoal = X_WOgoal @ X_OG\n    return X_WGgoal\n\n\ndef design_postgoal_pose(\n    X_WG: RigidTransform,\n) -> tuple[RigidTransform, RigidTransform, RigidTransform]:\n    X_GGApproach = RigidTransform([0.0, 0.0, -0.2])\n    X_WGApproach = X_WG @ X_GGApproach\n    return X_WGApproach\n\n\ndef make_trajectory(\n    X_Gs: list[RigidTransform], finger_values: np.ndarray, sample_times: list[float]\n) -> tuple[Trajectory, PiecewisePolynomial]:\n    robot_position_trajectory = PiecewisePose.MakeLinear(sample_times, X_Gs)\n    robot_velocity_trajectory = robot_position_trajectory.MakeDerivative()\n    traj_wsg_command = PiecewisePolynomial.FirstOrderHold(sample_times, finger_values)\n    return robot_velocity_trajectory, traj_wsg_command","block_group":"18ff217a5f4c456ca20b64c9b637f436","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cellId":"7d3a11fe519c4ea08c6e40a77acb8533","cell_type":"code","metadata":{"cell_id":"7d3a11fe519c4ea08c6e40a77acb8533","deepnote_cell_type":"code"},"source":"# TODO: copy over the pseudoinverse controller from the previous pset below.\n# OPTIONAL: if you want to you can try designing a more complicated controller for the pick and place.\n\n\nclass PseudoInverseController(LeafSystem):\n    def __init__(self, plant: MultibodyPlant):\n        raise NotImplementedError()\n\n    def CalcOutput(self, context: Context, output: BasicVector):\n        raise NotImplementedError()","block_group":"8ed7ddde81f6444d9c8f0073a577d24f","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cellId":"b1c799fc21374c2d98cf76ff75d4e42d","cell_type":"markdown","metadata":{"cell_id":"b1c799fc21374c2d98cf76ff75d4e42d","deepnote_cell_type":"markdown"},"source":"Now that we have our controller along with the poses of each of the objects, we can finally finish our pick and place. This section will also mainly be copying over code from the previous pset:","block_group":"f77119f2c12a4dd29128564eced8e90d"},{"cellId":"e67cdf9177f142c3a648577322ced45c","cell_type":"code","metadata":{"cell_id":"e67cdf9177f142c3a648577322ced45c","deepnote_cell_type":"code"},"source":"# we will rebuild the diagram in order to add the controller and integrator systems we need.\nbuilder, station = (\n    create_bimanual_IIWA14_with_table_and_initials_and_assets_and_cameras()\n)\nplant = station.GetSubsystemByName(\"plant\")\n\nstation_context = station.CreateDefaultContext()\nplant_context = plant.GetMyContextFromRoot(station_context)\n\n# get initial poses of gripper and objects\nX_WGinitial = plant.EvalBodyPoseInWorld(plant_context, plant.GetBodyByName(\"body\"))\n\n# TODO: copy over the poses registered from ICP above\nX_WOinitial = RigidTransform()  # TODO\n\n# Build trajectory keyframes\nX_OG, X_WGpick = design_grasp_pose(X_WOinitial)\nX_WGprepick = design_pregrasp_pose(X_WGpick)\nX_WGgoal = design_goal_poses(X_WOinitial, X_OG)\nX_WGpregoal = design_pregoal_pose(X_WGgoal)\nX_WGpostgoal = design_postgoal_pose(X_WGgoal)\n\n# constants for finger distances when the gripper is opened or closed\nopened = 0.107\nclosed = 0.0\n\n# list of keyframes, formatted as (gripper poses, finger states)\n# for each object the robot starts in its default pose with its gripper open\n# then it goes to the prepick pose, the pick pose, closes the gripper, and then goes\n# to the place pose\nkeyframes = [\n    (\"X_WGinitial\", X_WGinitial, opened),\n    (\"X_WGprepick\", X_WGprepick, opened),\n    (\"X_WGpick\", X_WGpick, opened),\n    (\"X_WGpick\", X_WGpick, closed),\n    (\"X_WGpregoal\", X_WGpregoal, closed),\n    (\"X_WGgoal\", X_WGgoal, closed),\n    (\"X_WGgoal\", X_WGgoal, opened),\n    (\"X_WGpostgoal\", X_WGpostgoal, opened),\n    (\"X_WGinitial\", X_WGinitial, opened),\n]\n\n# TODO: copy over your work from the previous pset\ngripper_poses = [keyframe[1] for keyframe in keyframes]\nfinger_states = np.asarray([keyframe[2] for keyframe in keyframes]).reshape(1, -1)\nsample_times = [3 * i for i in range(len(gripper_poses))]\ntraj_V_G, traj_wsg_command = make_trajectory(gripper_poses, finger_states, sample_times)\n\n# V_G_source defines a trajectory over gripper velocities. Add it to the system.\nV_G_source = builder.AddSystem(TrajectorySource(traj_V_G))\n# Add the DiffIK controller we just defined to the system\ncontroller = builder.AddSystem(PseudoInverseController(plant))\n# The HardwareStation expects robot commands in terms of joint angles.\n# We define the `integrator` system to map from joint_velocities to joint_angles.\nintegrator = builder.AddSystem(Integrator(7))\n# wsg_source defines a trajectory of finger positions. Add it to the system.\nwsg_source = builder.AddSystem(TrajectorySource(traj_wsg_command))\n\n# TODO: connect the joint velocity source to the pseudoinverse controller\n# TODO: connect the controller to integrator to get joint angle commands\n# TODO: connect the joint angles computed by the integrateor to the iiwa.position port on the manipulation station\n# TODO: connect the \"iiwa.position_measured\" port on the station back to the relevant input port on the controller\n# TODO: connect the wsg_source to the \"wsg.position\" input port of the station\n\n# visualize axes (useful for debugging)\nscenegraph = station.GetSubsystemByName(\"scene_graph\")\nAddFrameTriadIllustration(\n    scene_graph=scenegraph,\n    body=plant.GetBodyByName(f\"{your_initial}_body_link\"),\n    length=0.1,\n)\nAddFrameTriadIllustration(\n    scene_graph=scenegraph, body=plant.GetBodyByName(\"body\"), length=0.1\n)\n\ndiagram = builder.Build()","block_group":"7124564abf574b01b80eb37d5b2c537a","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cellId":"b6a1accfc5c3474a802ebe0cb98fec31","cell_type":"code","metadata":{"cell_id":"b6a1accfc5c3474a802ebe0cb98fec31","deepnote_cell_type":"code"},"source":"# Define the simulator\n\nsimulator = Simulator(diagram)\ncontext = simulator.get_mutable_context()\nstation_context = station.GetMyContextFromRoot(context)\nintegrator.set_integral_value(\n    integrator.GetMyContextFromRoot(context),\n    plant.GetPositions(\n        plant.GetMyContextFromRoot(context),\n        plant.GetModelInstanceByName(\"iiwa\"),\n    ),\n)\ndiagram.ForcedPublish(context)\nprint(f\"sanity check, simulation will run for {traj_V_G.end_time()} seconds\")\n\n# run simulation!\nmeshcat.StartRecording()\nprint(running_as_notebook)\nif running_as_notebook:\n    simulator.set_target_realtime_rate(1.0)\nsimulator.AdvanceTo(traj_V_G.end_time() if running_as_notebook else 0.01)\nmeshcat.StopRecording()\nmeshcat.PublishRecording()","block_group":"60e7bd9e9bee46fc896bfc80ab393709","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cellId":"f59057e7aa004c39891ab87bf73e1857","cell_type":"markdown","metadata":{"cell_id":"f59057e7aa004c39891ab87bf73e1857","deepnote_cell_type":"markdown"},"source":"Congratulations! If you've done everything right you should now have a fully end to end system that finds the geometry of the objects, registers the geometries with ICP, and picks them and places them upright. The final result should look something like below: \n\n![pick-and-place-geometry-initials-upright](https://raw.githubusercontent.com/RussTedrake/manipulation/master/book/figures/exercises/pick-place-geometry-initials-upright.png)\n\nAs in the previous pset, if the robot fails to grasp a letter, or if the letter seems to be slipping inside the robot's fingers, experiment with different grasp poses. Different letters may call for higher or lower grasps. \n","block_group":"5cd1bc9e0a8347738221c670912c85f3"},{"cellId":"a4837701c394456eae7d254b996f65eb","cell_type":"markdown","metadata":{"cell_id":"a4837701c394456eae7d254b996f65eb","deepnote_cell_type":"markdown"},"source":"As a challenge (for fun), try adding all of your initials to the scene and getting them upright without knocking the other letters over! We'll cover ways to do this (motion planning) in later chapters.\n\n**Once you have everything working, take a video and upload it to gradescope!**","block_group":"4ccd0c7916694a879fc0ceb2f3e27dd1"}],
        "metadata": {"deepnote_notebook_id":"313595febdf54466ad4b84b6aeb543a1"},
        "nbformat": "4",
        "nbformat_minor": "0",
        "version": "0"
      }